---
title: "Lecture RAG Assistant: Conversational Q&A Over Lecture Notes"

description: "A Retrieval-Augmented Generation (RAG) app for querying lecture notes via LangChain, ChromaDB, and LLMs like OpenAI."
date: 2025-07-30
categories: [Python, RAG, LLM, LangChain, Lecture Notes, Streamlit]
image: lecture_rag.png
format:
  html:
    toc: true
    number-sections: false
---

[![LangChain](https://img.shields.io/badge/LangChain-Enabled-blue)](https://www.langchain.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-Compatible-ff69b4)](https://platform.openai.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-orange)](https://streamlit.io/)
[![License](https://img.shields.io/github/license/your-org/lecture_rag)](https://github.com/your-org/lecture_rag/blob/main/LICENSE)

---

## 📚 Lecture RAG Assistant

**Lecture RAG Assistant** is a Retrieval-Augmented Generation (RAG) pipeline for querying lecture notes in `.ipynb` and `.pdf` format using a conversational interface powered by LLMs.

It combines the power of **LangChain**, **ChromaDB**, **HuggingFace Embeddings**, and **OpenAI models** (or local models) to generate accurate, source-grounded answers from your personal lecture material.

---

## Project Structure

```bash
lecture_rag/
├── app.py               # Streamlit dashboard for Q&A
├── ingest.py            # Extracts & chunks lecture files, saves to ChromaDB
├── qa_pipeline.py       # Loads retriever + LLM to create QA chain
├── lectures/            # Directory with lecture PDFs/Notebooks
├── chroma_langchain_db/ # Persisted vector DB (auto-created)
├── README.md            # Project documentation
```

---

## Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install langchain openai chromadb streamlit sentence-transformers PyMuPDF
```

### 2. Ingest Lecture Notes

Put `.pdf` and/or `.ipynb` files in the `./lectures/` folder and run:

```bash
python ingest.py
```

This will:

* Recursively scan the lecture folder
* Chunk the content intelligently
* Embed chunks with SentenceTransformers
* Save the vector DB in ChromaDB

### 3. Launch the Q&A Dashboard

```bash
streamlit run app.py
```

Use the web app to:

* Ask questions about your lecture content
* See grounded responses with source highlighting

---

## 🔑 API Key Setup

To use OpenAI models like `gpt-3.5-turbo`, set:

```bash
export OPENAI_API_KEY="your_openai_key_here"
```

If no key is found, the app skips LLM inference gracefully.

---

## 🧐 How It Works

### Ingestion (`ingest.py`)

* Loads `.ipynb` via `NotebookLoader`, `.pdf` via `PyPDFLoader`
* Splits text with `RecursiveCharacterTextSplitter`
* Embeds using `all-mpnet-base-v2`
* Stores in a persistent ChromaDB vector store

### Retrieval (`qa_pipeline.py`)

* Loads the vector DB as retriever
* Wraps LLM and retriever with `RetrievalQA` from LangChain

### Interaction (`app.py`)

* Accepts user question
* Fetches relevant chunks
* Constructs answer and shows references

---

## 🚀 Optional Features (TODO)

* Support for `.md` or `.html` files
* Upload feature in Streamlit
* Use local LLMs (e.g., Mistral, Llama.cpp)
* Rerank retrieved chunks for better answers

---

## 🙌 Credits

* Built with **LangChain**
* Embeddings via **SentenceTransformers**
* PDF parsing by **PyMuPDF**
* Interface via **Streamlit**

---

## 💬 Example Questions

* “What is a confounding variable?”
* “How does backpropagation work?”
* “What are the assumptions of linear regression?”

---

## 🔗 Learn More

Visit the [GitHub repository](https://github.com/vrudhgarg/lecture_search) for full setup, contribution guide, and demo walkthroughs.
